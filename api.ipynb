{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-21 22:29:57 api_server.py:177] vLLM API server version 0.5.0.post1\n",
      "INFO 06-21 22:29:57 api_server.py:178] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='./merged_model', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=2048, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, image_processor=None, image_processor_revision=None, disable_image_processor=False, scheduler_delay_factor=0.0, enable_chunked_prefill=False, speculative_model=None, num_speculative_tokens=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, model_loader_extra_config=None, preemption_mode=None, served_model_name=['Qwen2-1.5B-Instruct-lora'], qlora_adapter_name_or_path=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\n",
      "INFO 06-21 22:29:57 llm_engine.py:161] Initializing an LLM engine (v0.5.0.post1) with config: model='./merged_model', speculative_config=None, tokenizer='./merged_model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=Qwen2-1.5B-Instruct-lora)\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO 06-21 22:29:59 model_runner.py:160] Loading model weights took 2.8875 GB\n",
      "INFO 06-21 22:30:00 gpu_executor.py:83] # GPU blocks: 39646, # CPU blocks: 9362\n",
      "INFO 06-21 22:30:05 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-21 22:30:05 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-21 22:30:15 model_runner.py:965] Graph capturing finished in 9 secs.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING 06-21 22:30:15 serving_chat.py:95] No chat template provided. Chat API will not work.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING 06-21 22:30:16 serving_embedding.py:141] embedding_mode is False. Embedding API will not work.\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m20243\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
      "INFO 06-21 22:30:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:30:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:30:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:30:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:31:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:31:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:31:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:31:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:31:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:31:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:32:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:32:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:32:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:32:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:32:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:32:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:33:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:33:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:33:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:33:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:33:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:33:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:34:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:34:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:34:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:34:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:34:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:34:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:35:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:35:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:35:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:35:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:35:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:35:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:36:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:36:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:36:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:36:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:36:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:36:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:37:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:37:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:37:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:37:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:37:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:37:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:38:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:38:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:38:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:38:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:38:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n",
      "INFO 06-21 22:38:49 async_llm_engine.py:564] Received request cmpl-eed2c15ad4e94c9183e8e27c6234c4b6: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:38:51 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 52.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:38:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 150.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:39:01 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:39:02 async_llm_engine.py:133] Finished request cmpl-eed2c15ad4e94c9183e8e27c6234c4b6.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:39452 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:39:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:39:25 async_llm_engine.py:564] Received request cmpl-38b41777cfdb4522823e985ed431de29: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:39:25 metrics.py:341] Avg prompt throughput: 6.8 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:39:26 async_llm_engine.py:133] Finished request cmpl-38b41777cfdb4522823e985ed431de29.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:39:26 async_llm_engine.py:564] Received request cmpl-d4d27032455547ff8ec883f9ce2d1b0d: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:39:26 async_llm_engine.py:133] Finished request cmpl-d4d27032455547ff8ec883f9ce2d1b0d.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:39:26 async_llm_engine.py:564] Received request cmpl-0ebdfc36f6af4127b3ba47826995df27: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:39:30 metrics.py:341] Avg prompt throughput: 24.0 tokens/s, Avg generation throughput: 156.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:39:35 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:39:39 async_llm_engine.py:133] Finished request cmpl-0ebdfc36f6af4127b3ba47826995df27.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:39:39 async_llm_engine.py:564] Received request cmpl-f35c3db75dc8430f915f67e095c56553: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:39:40 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 154.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:39:45 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 151.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:39:50 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 151.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:39:52 async_llm_engine.py:133] Finished request cmpl-f35c3db75dc8430f915f67e095c56553.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:39:52 async_llm_engine.py:564] Received request cmpl-adc63a45500d4b4f98342eef1b537183: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:39:52 async_llm_engine.py:133] Finished request cmpl-adc63a45500d4b4f98342eef1b537183.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:39:52 async_llm_engine.py:564] Received request cmpl-43657d7cf23e498389bd9761022d427c: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:39:55 metrics.py:341] Avg prompt throughput: 24.0 tokens/s, Avg generation throughput: 150.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:40:00 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 152.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:40:05 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 152.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:40:05 async_llm_engine.py:133] Finished request cmpl-43657d7cf23e498389bd9761022d427c.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:40:06 async_llm_engine.py:564] Received request cmpl-78b6ff359dc04412818e613c1be7d94a: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:40:10 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 156.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:40:15 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:40:18 async_llm_engine.py:133] Finished request cmpl-78b6ff359dc04412818e613c1be7d94a.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:40:18 async_llm_engine.py:564] Received request cmpl-50b76fbf43044480b17d285a2c82e637: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:40:20 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 155.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:40:25 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:40:30 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:40:31 async_llm_engine.py:133] Finished request cmpl-50b76fbf43044480b17d285a2c82e637.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:40:31 async_llm_engine.py:564] Received request cmpl-a385c3e3747b4953ad7e328c58783e22: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:40:35 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 156.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:40:40 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:40:44 async_llm_engine.py:133] Finished request cmpl-a385c3e3747b4953ad7e328c58783e22.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:40:44 async_llm_engine.py:564] Received request cmpl-9519bd27a97740ebaa528e56d133f8da: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:40:45 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 155.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:40:50 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:40:55 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:40:56 async_llm_engine.py:133] Finished request cmpl-9519bd27a97740ebaa528e56d133f8da.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:40:56 async_llm_engine.py:564] Received request cmpl-2c3a844926cf4beb8d111676a05a4cd1: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:40:57 async_llm_engine.py:133] Finished request cmpl-2c3a844926cf4beb8d111676a05a4cd1.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:40:57 async_llm_engine.py:564] Received request cmpl-b07d502cb12f4d1e83ea67623e857556: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:41:00 metrics.py:341] Avg prompt throughput: 24.0 tokens/s, Avg generation throughput: 155.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:41:05 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:41:09 async_llm_engine.py:133] Finished request cmpl-b07d502cb12f4d1e83ea67623e857556.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:41:09 async_llm_engine.py:564] Received request cmpl-54214b64850f428e9665f374ec51aa4a: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:41:10 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 155.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:41:11 async_llm_engine.py:133] Finished request cmpl-54214b64850f428e9665f374ec51aa4a.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:41:11 async_llm_engine.py:564] Received request cmpl-662b107ec3cb4184a9f47f68c3a03001: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:41:15 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 157.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:41:20 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:41:24 async_llm_engine.py:133] Finished request cmpl-662b107ec3cb4184a9f47f68c3a03001.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:41:24 async_llm_engine.py:564] Received request cmpl-c7cdd937009b4360b6b71b09f05703cd: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:41:25 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 155.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:41:30 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:41:35 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:41:36 async_llm_engine.py:133] Finished request cmpl-c7cdd937009b4360b6b71b09f05703cd.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:41:36 async_llm_engine.py:564] Received request cmpl-7a0650d927d64c85af5d88087c314c65: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:41:40 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 157.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:41:45 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:41:49 async_llm_engine.py:133] Finished request cmpl-7a0650d927d64c85af5d88087c314c65.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:41:49 async_llm_engine.py:564] Received request cmpl-630ec25b33fa46c1b6adbf93bb32165f: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:41:50 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 155.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:41:50 async_llm_engine.py:133] Finished request cmpl-630ec25b33fa46c1b6adbf93bb32165f.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:41:50 async_llm_engine.py:564] Received request cmpl-76ce5e33b5ce40dd9eb0f274c5366505: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:41:52 async_llm_engine.py:133] Finished request cmpl-76ce5e33b5ce40dd9eb0f274c5366505.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:41:52 async_llm_engine.py:564] Received request cmpl-8d894afa9f464b74acc92c9551ad7922: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:41:55 metrics.py:341] Avg prompt throughput: 24.0 tokens/s, Avg generation throughput: 157.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:42:00 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:42:04 async_llm_engine.py:133] Finished request cmpl-8d894afa9f464b74acc92c9551ad7922.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:42:04 async_llm_engine.py:564] Received request cmpl-8d4dfdbaa90740f3914efca68d4b27bb: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:42:05 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 155.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:42:10 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:42:15 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:42:17 async_llm_engine.py:133] Finished request cmpl-8d4dfdbaa90740f3914efca68d4b27bb.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:42:17 async_llm_engine.py:564] Received request cmpl-e56e52fa855d46aa85120570914cdbe7: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:42:20 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 156.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:42:25 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:42:30 async_llm_engine.py:133] Finished request cmpl-e56e52fa855d46aa85120570914cdbe7.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:42:30 async_llm_engine.py:564] Received request cmpl-2b0ccfd250fb4a4eb15939c4f8b08320: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:42:30 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 154.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:42:35 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:42:40 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:42:42 async_llm_engine.py:133] Finished request cmpl-2b0ccfd250fb4a4eb15939c4f8b08320.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:42:42 async_llm_engine.py:564] Received request cmpl-6986265a74924cf09a011b82f9febac9: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:42:45 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 156.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:42:50 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:42:55 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:42:55 async_llm_engine.py:133] Finished request cmpl-6986265a74924cf09a011b82f9febac9.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:42:55 async_llm_engine.py:564] Received request cmpl-9d5accdaf1404e6cbd3cea7013e48f2c: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:43:00 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 156.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:43:05 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:43:08 async_llm_engine.py:133] Finished request cmpl-9d5accdaf1404e6cbd3cea7013e48f2c.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:43:08 async_llm_engine.py:564] Received request cmpl-61b815cf45514829bdaf2729e69e3966: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:43:10 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 157.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:43:11 async_llm_engine.py:133] Finished request cmpl-61b815cf45514829bdaf2729e69e3966.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:43:11 async_llm_engine.py:564] Received request cmpl-4e1914842b284b5289322c06200f7551: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:43:12 async_llm_engine.py:133] Finished request cmpl-4e1914842b284b5289322c06200f7551.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:43:12 async_llm_engine.py:564] Received request cmpl-88ff57ffcd354617b396bf55fbbdd18f: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:43:13 async_llm_engine.py:133] Finished request cmpl-88ff57ffcd354617b396bf55fbbdd18f.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:43:13 async_llm_engine.py:564] Received request cmpl-dc98a638041d4f37a0a37fa23de0f74c: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:43:15 metrics.py:341] Avg prompt throughput: 36.0 tokens/s, Avg generation throughput: 156.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:43:17 async_llm_engine.py:133] Finished request cmpl-dc98a638041d4f37a0a37fa23de0f74c.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:43:17 async_llm_engine.py:564] Received request cmpl-bb065cbe19614cc6b37f6e86af80a596: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:43:20 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 156.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:43:25 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 159.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:43:30 async_llm_engine.py:133] Finished request cmpl-bb065cbe19614cc6b37f6e86af80a596.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:43:30 async_llm_engine.py:564] Received request cmpl-86dacf075d32497abb668391078120f4: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:43:30 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 156.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:43:35 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:43:40 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:43:42 async_llm_engine.py:133] Finished request cmpl-86dacf075d32497abb668391078120f4.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:43:42 async_llm_engine.py:564] Received request cmpl-762383c849384a028466b465b0addfa9: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:43:45 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 154.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:43:50 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:43:55 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 159.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:43:55 async_llm_engine.py:133] Finished request cmpl-762383c849384a028466b465b0addfa9.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:43:55 async_llm_engine.py:564] Received request cmpl-b4fa7b51cc5c4fd9a4af9324b828e8f9: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:44:00 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 156.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:44:05 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:44:08 async_llm_engine.py:133] Finished request cmpl-b4fa7b51cc5c4fd9a4af9324b828e8f9.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:44:08 async_llm_engine.py:564] Received request cmpl-c814912b7539464ab68019b817c563bc: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:44:10 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 155.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:44:15 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:44:20 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:44:20 async_llm_engine.py:133] Finished request cmpl-c814912b7539464ab68019b817c563bc.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:44:21 async_llm_engine.py:564] Received request cmpl-0357de00d83d45de9923085e98c59420: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:44:25 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 156.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:44:30 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:44:33 async_llm_engine.py:133] Finished request cmpl-0357de00d83d45de9923085e98c59420.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:44:33 async_llm_engine.py:564] Received request cmpl-5254a0fdac0f40888aaa6f3974facb79: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:44:35 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 155.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:44:40 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:44:45 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:44:46 async_llm_engine.py:133] Finished request cmpl-5254a0fdac0f40888aaa6f3974facb79.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:44:46 async_llm_engine.py:564] Received request cmpl-e0e7bf2125954b6c805693e975d48e79: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:44:47 async_llm_engine.py:133] Finished request cmpl-e0e7bf2125954b6c805693e975d48e79.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:44:47 async_llm_engine.py:564] Received request cmpl-44122f8a61dc42da8ade01e6e1debf17: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:44:50 metrics.py:341] Avg prompt throughput: 24.0 tokens/s, Avg generation throughput: 156.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:44:55 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:44:59 async_llm_engine.py:133] Finished request cmpl-44122f8a61dc42da8ade01e6e1debf17.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:44:59 async_llm_engine.py:564] Received request cmpl-6de7241f35e947ff8cd0b8d53835265e: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:45:00 async_llm_engine.py:133] Finished request cmpl-6de7241f35e947ff8cd0b8d53835265e.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:45:00 async_llm_engine.py:564] Received request cmpl-c111bd3b33934909b154b4c7a485e1ad: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:45:00 metrics.py:341] Avg prompt throughput: 24.0 tokens/s, Avg generation throughput: 157.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:45:00 async_llm_engine.py:133] Finished request cmpl-c111bd3b33934909b154b4c7a485e1ad.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:45:00 async_llm_engine.py:564] Received request cmpl-43ab8736840a4507a7fb18ff148d7121: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:45:05 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 158.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:45:10 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:45:13 async_llm_engine.py:133] Finished request cmpl-43ab8736840a4507a7fb18ff148d7121.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:45:13 async_llm_engine.py:564] Received request cmpl-9eb576387f334ba4ac3af9fd3a50276c: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:45:15 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 155.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:45:20 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:45:25 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:45:25 async_llm_engine.py:133] Finished request cmpl-9eb576387f334ba4ac3af9fd3a50276c.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:45:25 async_llm_engine.py:564] Received request cmpl-492920fcd7854bac80b8b876227138cc: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:45:30 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 155.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:45:35 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:45:38 async_llm_engine.py:133] Finished request cmpl-492920fcd7854bac80b8b876227138cc.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:45:38 async_llm_engine.py:564] Received request cmpl-32cbc603c58246d9b734c79a246bfce3: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:45:40 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 154.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:45:45 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:45:50 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 151.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:45:51 async_llm_engine.py:133] Finished request cmpl-32cbc603c58246d9b734c79a246bfce3.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:45:51 async_llm_engine.py:564] Received request cmpl-46e9b27082984e03a724a4192f510172: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:45:55 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 153.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:46:00 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 151.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:46:04 async_llm_engine.py:133] Finished request cmpl-46e9b27082984e03a724a4192f510172.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:46:04 async_llm_engine.py:564] Received request cmpl-39cd3d72b6ab4324829bcd13f63aa5a9: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:46:05 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 157.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:46:10 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 160.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:46:15 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:46:17 async_llm_engine.py:133] Finished request cmpl-39cd3d72b6ab4324829bcd13f63aa5a9.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:46:17 async_llm_engine.py:564] Received request cmpl-29d809af39c348d4adff92f2ef124d34: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:46:20 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 158.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:46:25 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:46:29 async_llm_engine.py:133] Finished request cmpl-29d809af39c348d4adff92f2ef124d34.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:46:29 async_llm_engine.py:564] Received request cmpl-1dd1d13dc93d46e792b165dbfa7be172: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:46:30 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 158.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:46:35 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 163.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:46:40 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:46:42 async_llm_engine.py:133] Finished request cmpl-1dd1d13dc93d46e792b165dbfa7be172.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:46:42 async_llm_engine.py:564] Received request cmpl-53fa3846de0f47dca95c76bc517cf16a: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:46:45 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 156.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:46:50 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:46:54 async_llm_engine.py:133] Finished request cmpl-53fa3846de0f47dca95c76bc517cf16a.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:46:54 async_llm_engine.py:564] Received request cmpl-539377c1a76d482c8dbf6c6ee8a4c797: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:46:55 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 155.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:47:00 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:47:05 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 152.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:47:07 async_llm_engine.py:133] Finished request cmpl-539377c1a76d482c8dbf6c6ee8a4c797.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:47:07 async_llm_engine.py:564] Received request cmpl-8764656ba4f143708871067f3bebae8b: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代励志故事，一个失业青年如何克服生活困境，终于实现自我突破，成为行业翘楚的心路历程。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 109880, 101108, 3837, 46944, 108127, 103970, 100007, 106268, 99424, 106499, 3837, 104020, 101884, 104049, 101969, 3837, 99787, 99717, 109416, 100785, 101421, 45995, 103168, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:47:10 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 150.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:47:15 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:47:20 async_llm_engine.py:133] Finished request cmpl-8764656ba4f143708871067f3bebae8b.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34704 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:47:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:47:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:47:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:47:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:48:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:48:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:48:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:48:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:48:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:48:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:49:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:49:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:49:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:49:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:49:41 async_llm_engine.py:564] Received request cmpl-fa91dd9a20004bdd881b4ca77c9cce13: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:49:41 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 4.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:49:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:49:51 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:49:53 async_llm_engine.py:133] Finished request cmpl-fa91dd9a20004bdd881b4ca77c9cce13.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:49:53 async_llm_engine.py:564] Received request cmpl-09a1dabcc3054e7390238ef31754fc42: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:49:55 async_llm_engine.py:133] Finished request cmpl-09a1dabcc3054e7390238ef31754fc42.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:49:55 async_llm_engine.py:564] Received request cmpl-d170d9d25a384a6cbbe6c2b7d3645b87: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:49:56 metrics.py:341] Avg prompt throughput: 19.6 tokens/s, Avg generation throughput: 155.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:49:57 async_llm_engine.py:133] Finished request cmpl-d170d9d25a384a6cbbe6c2b7d3645b87.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:49:57 async_llm_engine.py:564] Received request cmpl-7b3395770c944fda8c9ab8a24b1d3221: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:50:01 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 158.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:50:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 159.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:50:09 async_llm_engine.py:133] Finished request cmpl-7b3395770c944fda8c9ab8a24b1d3221.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:50:09 async_llm_engine.py:564] Received request cmpl-b7373c5cba8b4749babe38fbb2d2b9e6: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:50:11 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 158.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:50:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 163.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:50:21 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 160.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:50:22 async_llm_engine.py:133] Finished request cmpl-b7373c5cba8b4749babe38fbb2d2b9e6.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:50:22 async_llm_engine.py:564] Received request cmpl-2f2ffaac02f34331a4507036261020e2: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:50:26 async_llm_engine.py:133] Finished request cmpl-2f2ffaac02f34331a4507036261020e2.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:50:26 async_llm_engine.py:564] Received request cmpl-eb0aaa43148f42599791b74aa79fc853: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:50:26 metrics.py:341] Avg prompt throughput: 19.6 tokens/s, Avg generation throughput: 160.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:50:31 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 160.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:50:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:50:38 async_llm_engine.py:133] Finished request cmpl-eb0aaa43148f42599791b74aa79fc853.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:50:38 async_llm_engine.py:564] Received request cmpl-366540c89f6d43e38790207211d3e60c: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:50:41 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 156.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:50:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:50:51 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 160.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:50:51 async_llm_engine.py:133] Finished request cmpl-366540c89f6d43e38790207211d3e60c.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:50:51 async_llm_engine.py:564] Received request cmpl-adbfd4d7470641e3adb95a9ef0fe9564: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:50:56 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 161.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:51:01 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:51:04 async_llm_engine.py:133] Finished request cmpl-adbfd4d7470641e3adb95a9ef0fe9564.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:51:04 async_llm_engine.py:564] Received request cmpl-b95f4c13ead14556a147c498ca4d2f43: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:51:06 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 159.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:51:11 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 159.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:51:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:51:16 async_llm_engine.py:133] Finished request cmpl-b95f4c13ead14556a147c498ca4d2f43.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:51:16 async_llm_engine.py:564] Received request cmpl-764676f10a2d419abfd7e3476c75f3be: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:51:17 async_llm_engine.py:133] Finished request cmpl-764676f10a2d419abfd7e3476c75f3be.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:51:17 async_llm_engine.py:564] Received request cmpl-239569d0a72a406a84952ea2ac556d7e: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:51:21 metrics.py:341] Avg prompt throughput: 19.6 tokens/s, Avg generation throughput: 158.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:51:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 162.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:51:30 async_llm_engine.py:133] Finished request cmpl-239569d0a72a406a84952ea2ac556d7e.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:51:30 async_llm_engine.py:564] Received request cmpl-bc73a92a32344ef29823688b26250c19: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:51:31 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 158.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:51:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 162.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:51:41 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:51:42 async_llm_engine.py:133] Finished request cmpl-bc73a92a32344ef29823688b26250c19.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:51:42 async_llm_engine.py:564] Received request cmpl-30c56d87082d4a1686b5dc8aa3017325: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:51:44 async_llm_engine.py:133] Finished request cmpl-30c56d87082d4a1686b5dc8aa3017325.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:51:44 async_llm_engine.py:564] Received request cmpl-af023130b4fe414bb686205ecb964df9: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:51:46 metrics.py:341] Avg prompt throughput: 19.6 tokens/s, Avg generation throughput: 159.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:51:51 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 160.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:51:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 159.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:51:56 async_llm_engine.py:133] Finished request cmpl-af023130b4fe414bb686205ecb964df9.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:51:56 async_llm_engine.py:564] Received request cmpl-c4c7f01ae1b7450db4c58936d5d865d3: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:52:01 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 156.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:52:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:52:09 async_llm_engine.py:133] Finished request cmpl-c4c7f01ae1b7450db4c58936d5d865d3.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:52:09 async_llm_engine.py:564] Received request cmpl-c7eb1e6c5307445abe56e8eb4af759d2: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:52:11 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 155.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:52:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:52:21 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:52:22 async_llm_engine.py:133] Finished request cmpl-c7eb1e6c5307445abe56e8eb4af759d2.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:52:22 async_llm_engine.py:564] Received request cmpl-e6004ea2a9f849fc8b852dc21bfcd97b: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:52:26 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 158.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:52:31 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 159.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:52:33 async_llm_engine.py:133] Finished request cmpl-e6004ea2a9f849fc8b852dc21bfcd97b.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:52:33 async_llm_engine.py:564] Received request cmpl-0cc65b5033c747ad82838369ff84a437: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:52:36 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 156.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:52:41 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:52:46 async_llm_engine.py:133] Finished request cmpl-0cc65b5033c747ad82838369ff84a437.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:52:46 async_llm_engine.py:564] Received request cmpl-6b424d4eb55f483bb5ed2da1b7f9de46: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:52:46 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 154.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:52:51 async_llm_engine.py:133] Finished request cmpl-6b424d4eb55f483bb5ed2da1b7f9de46.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:52:51 async_llm_engine.py:564] Received request cmpl-f8d79d3c193041bc92bdbb8609258d69: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:52:51 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 156.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:52:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:53:01 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:53:03 async_llm_engine.py:133] Finished request cmpl-f8d79d3c193041bc92bdbb8609258d69.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:53:03 async_llm_engine.py:564] Received request cmpl-ec78ac08f7c04f5e88f984c254a70aa9: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:53:06 async_llm_engine.py:133] Finished request cmpl-ec78ac08f7c04f5e88f984c254a70aa9.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:53:06 async_llm_engine.py:564] Received request cmpl-9fd19aaa5f4043dbaab2e4e435ba6075: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:53:06 metrics.py:341] Avg prompt throughput: 19.6 tokens/s, Avg generation throughput: 154.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:53:11 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:53:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:53:19 async_llm_engine.py:133] Finished request cmpl-9fd19aaa5f4043dbaab2e4e435ba6075.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:53:19 async_llm_engine.py:564] Received request cmpl-deac1f87a57748a88fc5d525c16345a7: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:53:20 async_llm_engine.py:133] Finished request cmpl-deac1f87a57748a88fc5d525c16345a7.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:53:20 async_llm_engine.py:564] Received request cmpl-fce9f29126154e8daec67fd2d404fa25: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:53:21 metrics.py:341] Avg prompt throughput: 19.6 tokens/s, Avg generation throughput: 154.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:53:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:53:31 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:53:33 async_llm_engine.py:133] Finished request cmpl-fce9f29126154e8daec67fd2d404fa25.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:53:33 async_llm_engine.py:564] Received request cmpl-12fdaf49607f463aaa9abaab941901e1: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:53:36 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 154.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:53:41 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:53:45 async_llm_engine.py:133] Finished request cmpl-12fdaf49607f463aaa9abaab941901e1.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:53:45 async_llm_engine.py:564] Received request cmpl-a63dcd9b72674ec9b9fb8271411ec2f4: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:53:46 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 154.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:53:51 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:53:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:53:58 async_llm_engine.py:133] Finished request cmpl-a63dcd9b72674ec9b9fb8271411ec2f4.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:53:58 async_llm_engine.py:564] Received request cmpl-69636b5873a54028a17943e1744a2ad3: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:54:01 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 148.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:54:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:54:11 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:54:11 async_llm_engine.py:133] Finished request cmpl-69636b5873a54028a17943e1744a2ad3.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:54:11 async_llm_engine.py:564] Received request cmpl-5b4d6b168c8c46f2b3ae27b357c58159: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:54:16 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 157.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:54:21 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:54:24 async_llm_engine.py:133] Finished request cmpl-5b4d6b168c8c46f2b3ae27b357c58159.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:54:24 async_llm_engine.py:564] Received request cmpl-ef459eea36c4426f92ac78d2662fdb4e: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:54:26 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 154.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:54:31 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:54:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:54:37 async_llm_engine.py:133] Finished request cmpl-ef459eea36c4426f92ac78d2662fdb4e.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:54:37 async_llm_engine.py:564] Received request cmpl-c3ee4830b31e47ee95e613b2c10515b1: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:54:41 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 153.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:54:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 153.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:54:50 async_llm_engine.py:133] Finished request cmpl-c3ee4830b31e47ee95e613b2c10515b1.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:54:50 async_llm_engine.py:564] Received request cmpl-7fed949c3b5045f1932106d540ae2ad0: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:54:50 async_llm_engine.py:133] Finished request cmpl-7fed949c3b5045f1932106d540ae2ad0.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:54:50 async_llm_engine.py:564] Received request cmpl-68a85b767ac243f5ad0071b99bc1bde5: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:54:51 metrics.py:341] Avg prompt throughput: 19.6 tokens/s, Avg generation throughput: 151.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:54:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:55:01 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:55:03 async_llm_engine.py:133] Finished request cmpl-68a85b767ac243f5ad0071b99bc1bde5.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:55:03 async_llm_engine.py:564] Received request cmpl-b9cd415818c54c45a3d1e9a148da8832: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:55:06 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 156.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:55:11 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:55:16 async_llm_engine.py:133] Finished request cmpl-b9cd415818c54c45a3d1e9a148da8832.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:55:16 async_llm_engine.py:564] Received request cmpl-d364f3c14c2a4db0b99537b742cab422: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:55:16 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 156.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:55:21 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:55:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:55:29 async_llm_engine.py:133] Finished request cmpl-d364f3c14c2a4db0b99537b742cab422.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:55:29 async_llm_engine.py:564] Received request cmpl-d7652f01b7de48e99a7bc64d09101be1: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:55:31 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 156.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:55:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:55:41 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:55:41 async_llm_engine.py:133] Finished request cmpl-d7652f01b7de48e99a7bc64d09101be1.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:55:41 async_llm_engine.py:564] Received request cmpl-a8b89f4df8f34cb0a80439f52c8f715b: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:55:46 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 157.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:55:51 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:55:54 async_llm_engine.py:133] Finished request cmpl-a8b89f4df8f34cb0a80439f52c8f715b.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:55:54 async_llm_engine.py:564] Received request cmpl-82bddd1e88d942de91ee3470ce169688: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:55:56 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 154.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:56:01 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:56:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:56:07 async_llm_engine.py:133] Finished request cmpl-82bddd1e88d942de91ee3470ce169688.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:56:07 async_llm_engine.py:564] Received request cmpl-b2410e9201ea482e82d054cc9df54444: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:56:11 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 156.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:56:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:56:20 async_llm_engine.py:133] Finished request cmpl-b2410e9201ea482e82d054cc9df54444.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:56:20 async_llm_engine.py:564] Received request cmpl-0086dc0bb2154cc5a378079c78583a0a: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:56:21 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 154.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:56:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:56:31 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:56:32 async_llm_engine.py:133] Finished request cmpl-0086dc0bb2154cc5a378079c78583a0a.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:56:32 async_llm_engine.py:564] Received request cmpl-ee18b23c75ba44a69a0b33d423713bc6: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:56:36 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 156.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:56:41 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:56:45 async_llm_engine.py:133] Finished request cmpl-ee18b23c75ba44a69a0b33d423713bc6.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:56:45 async_llm_engine.py:564] Received request cmpl-732a8967dc4f4aeb83e9c900dcae31eb: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:56:46 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 154.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:56:51 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:56:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:56:58 async_llm_engine.py:133] Finished request cmpl-732a8967dc4f4aeb83e9c900dcae31eb.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:56:58 async_llm_engine.py:564] Received request cmpl-3202ddf2b4e64cd292c54fa6282c5fc4: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:57:00 async_llm_engine.py:133] Finished request cmpl-3202ddf2b4e64cd292c54fa6282c5fc4.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:57:00 async_llm_engine.py:564] Received request cmpl-3a23a3117c81410985b2af9026aabe41: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:57:01 metrics.py:341] Avg prompt throughput: 19.6 tokens/s, Avg generation throughput: 150.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:57:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 154.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:57:11 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:57:13 async_llm_engine.py:133] Finished request cmpl-3a23a3117c81410985b2af9026aabe41.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:57:13 async_llm_engine.py:564] Received request cmpl-6b4b924a277b4e45bc0f0710afaa7b21: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:57:16 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 149.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:57:21 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 151.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:57:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 154.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:57:26 async_llm_engine.py:133] Finished request cmpl-6b4b924a277b4e45bc0f0710afaa7b21.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:57:26 async_llm_engine.py:564] Received request cmpl-9aefd18baaea44c6870d1c30b86d9219: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:57:30 async_llm_engine.py:133] Finished request cmpl-9aefd18baaea44c6870d1c30b86d9219.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:57:30 async_llm_engine.py:564] Received request cmpl-1832450cd3524c9fa4cb44ded9eca14f: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:57:30 async_llm_engine.py:133] Finished request cmpl-1832450cd3524c9fa4cb44ded9eca14f.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:57:30 async_llm_engine.py:564] Received request cmpl-e282ceea1de84f8bb6b2266d916cfabe: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:57:31 metrics.py:341] Avg prompt throughput: 29.4 tokens/s, Avg generation throughput: 151.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:57:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:57:41 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:57:43 async_llm_engine.py:133] Finished request cmpl-e282ceea1de84f8bb6b2266d916cfabe.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:57:43 async_llm_engine.py:564] Received request cmpl-1c8c33ca2fe243a2846822ba1483b5a1: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:57:46 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 151.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:57:51 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:57:56 async_llm_engine.py:133] Finished request cmpl-1c8c33ca2fe243a2846822ba1483b5a1.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:57:56 async_llm_engine.py:564] Received request cmpl-c8efc92a00a44b9880c9b88f901e614c: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n一个现代女性穿越到古代某朝代后发生的传奇故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1999, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 46944, 100390, 101968, 106063, 26939, 102640, 99569, 99816, 30540, 33447, 106806, 99926, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:57:56 metrics.py:341] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 154.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:58:01 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:58:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:58:09 async_llm_engine.py:133] Finished request cmpl-c8efc92a00a44b9880c9b88f901e614c.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:58:09 async_llm_engine.py:564] Received request cmpl-e57d67c7a0bb494bbc9d33cdff762146: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:58:11 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 155.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:58:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:58:21 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 154.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:58:21 async_llm_engine.py:133] Finished request cmpl-e57d67c7a0bb494bbc9d33cdff762146.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:58:21 async_llm_engine.py:564] Received request cmpl-a3733118f733415381e13fd69c1d9a27: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:58:24 async_llm_engine.py:133] Finished request cmpl-a3733118f733415381e13fd69c1d9a27.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:58:24 async_llm_engine.py:564] Received request cmpl-c2e9cd313d394311bf99cb8c1b25a7b7: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:58:26 metrics.py:341] Avg prompt throughput: 23.2 tokens/s, Avg generation throughput: 155.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:58:31 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:58:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:58:36 async_llm_engine.py:133] Finished request cmpl-c2e9cd313d394311bf99cb8c1b25a7b7.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:58:36 async_llm_engine.py:564] Received request cmpl-b56c23c085d846cd9ccaa74d64c51d1b: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:58:41 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 155.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:58:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:58:49 async_llm_engine.py:133] Finished request cmpl-b56c23c085d846cd9ccaa74d64c51d1b.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:58:49 async_llm_engine.py:564] Received request cmpl-7f7824b9634c490c89f4f82cfe1972a4: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:58:51 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 155.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:58:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:59:01 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:59:02 async_llm_engine.py:133] Finished request cmpl-7f7824b9634c490c89f4f82cfe1972a4.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:59:02 async_llm_engine.py:564] Received request cmpl-917b3464efcd441a8f71de5340d7b4cb: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:59:06 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 156.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:59:11 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:59:15 async_llm_engine.py:133] Finished request cmpl-917b3464efcd441a8f71de5340d7b4cb.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:59:15 async_llm_engine.py:564] Received request cmpl-9649e869c0f54f0a925131cd5ff59e9e: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:59:16 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 154.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:59:21 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:59:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:59:27 async_llm_engine.py:133] Finished request cmpl-9649e869c0f54f0a925131cd5ff59e9e.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:59:27 async_llm_engine.py:564] Received request cmpl-819aa5a1bf854a75a28ea68665747aa1: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:59:31 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 159.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:59:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 159.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:59:40 async_llm_engine.py:133] Finished request cmpl-819aa5a1bf854a75a28ea68665747aa1.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:59:40 async_llm_engine.py:564] Received request cmpl-81ea22d246654fc287347d2b55947a63: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:59:41 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 154.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:59:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 152.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:59:51 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 154.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:59:53 async_llm_engine.py:133] Finished request cmpl-81ea22d246654fc287347d2b55947a63.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:59:53 async_llm_engine.py:564] Received request cmpl-ab774128d6a64d03bccd9e9742a749b3: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:59:55 async_llm_engine.py:133] Finished request cmpl-ab774128d6a64d03bccd9e9742a749b3.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:59:55 async_llm_engine.py:564] Received request cmpl-64c57beb06174df1bb7a8366b83e5b5a: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:59:55 async_llm_engine.py:133] Finished request cmpl-64c57beb06174df1bb7a8366b83e5b5a.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:59:55 async_llm_engine.py:564] Received request cmpl-533b09703de2408395c3b05ef80d76c4: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 22:59:56 metrics.py:341] Avg prompt throughput: 34.8 tokens/s, Avg generation throughput: 151.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 22:59:59 async_llm_engine.py:133] Finished request cmpl-533b09703de2408395c3b05ef80d76c4.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 22:59:59 async_llm_engine.py:564] Received request cmpl-7ef54e70148f47bf9864347eb18e9154: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:00:01 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 156.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:00:04 async_llm_engine.py:133] Finished request cmpl-7ef54e70148f47bf9864347eb18e9154.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:00:04 async_llm_engine.py:564] Received request cmpl-3f0382ff736c4fdb92a1e3a53cd3cf82: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:00:06 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 158.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:00:11 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 159.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:00:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:00:17 async_llm_engine.py:133] Finished request cmpl-3f0382ff736c4fdb92a1e3a53cd3cf82.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:00:17 async_llm_engine.py:564] Received request cmpl-de8f2bf36745481a9b507e198708c04a: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:00:21 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 156.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:00:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:00:30 async_llm_engine.py:133] Finished request cmpl-de8f2bf36745481a9b507e198708c04a.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:00:30 async_llm_engine.py:564] Received request cmpl-281316514d3f4e8e9bd773d975a55acb: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:00:31 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 154.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:00:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:00:41 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 154.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:00:42 async_llm_engine.py:133] Finished request cmpl-281316514d3f4e8e9bd773d975a55acb.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:00:42 async_llm_engine.py:564] Received request cmpl-45ff0573d45a46bca78f957b639ba3c1: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:00:46 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 156.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:00:51 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:00:55 async_llm_engine.py:133] Finished request cmpl-45ff0573d45a46bca78f957b639ba3c1.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:00:55 async_llm_engine.py:564] Received request cmpl-1376160940be498594b054f4bae222e7: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:00:56 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 155.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:01:01 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:01:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:01:08 async_llm_engine.py:133] Finished request cmpl-1376160940be498594b054f4bae222e7.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:01:08 async_llm_engine.py:564] Received request cmpl-809863a553d845938773b4c6c8989764: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:01:11 async_llm_engine.py:133] Finished request cmpl-809863a553d845938773b4c6c8989764.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:01:11 async_llm_engine.py:564] Received request cmpl-2b488d154bea453d89ca3e1645e39a5b: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:01:11 metrics.py:341] Avg prompt throughput: 23.1 tokens/s, Avg generation throughput: 155.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:01:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:01:21 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:01:24 async_llm_engine.py:133] Finished request cmpl-2b488d154bea453d89ca3e1645e39a5b.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:01:24 async_llm_engine.py:564] Received request cmpl-b589452415ed4937b04781964e126026: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:01:26 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 155.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:01:31 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:01:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:01:37 async_llm_engine.py:133] Finished request cmpl-b589452415ed4937b04781964e126026.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:01:37 async_llm_engine.py:564] Received request cmpl-5bff8b0a5e004ba8af347291a9afe5b5: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:01:41 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 155.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:01:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:01:49 async_llm_engine.py:133] Finished request cmpl-5bff8b0a5e004ba8af347291a9afe5b5.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:01:49 async_llm_engine.py:564] Received request cmpl-5f709a89a39046b09e6f4300e4e66b0a: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:01:51 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 156.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:01:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:02:01 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:02:02 async_llm_engine.py:133] Finished request cmpl-5f709a89a39046b09e6f4300e4e66b0a.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:02:02 async_llm_engine.py:564] Received request cmpl-5424bc8bca054b049306939fc53c45c7: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:02:06 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 156.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:02:07 async_llm_engine.py:133] Finished request cmpl-5424bc8bca054b049306939fc53c45c7.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:02:07 async_llm_engine.py:564] Received request cmpl-2dc8d113afaf48cd8405eef9a107632d: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:02:11 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 157.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:02:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:02:20 async_llm_engine.py:133] Finished request cmpl-2dc8d113afaf48cd8405eef9a107632d.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:02:20 async_llm_engine.py:564] Received request cmpl-9375cee955184917a15a3fd952521c97: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:02:21 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 154.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:02:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:02:31 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:02:33 async_llm_engine.py:133] Finished request cmpl-9375cee955184917a15a3fd952521c97.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:02:33 async_llm_engine.py:564] Received request cmpl-83e016d12cc148e69256caf1ef096336: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:02:36 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 156.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:02:41 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:02:45 async_llm_engine.py:133] Finished request cmpl-83e016d12cc148e69256caf1ef096336.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:02:45 async_llm_engine.py:564] Received request cmpl-a121fa319e764f878d5a4396e8ed5e0c: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:02:46 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 155.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:02:51 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:02:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:02:58 async_llm_engine.py:133] Finished request cmpl-a121fa319e764f878d5a4396e8ed5e0c.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:02:58 async_llm_engine.py:564] Received request cmpl-14eab5ce1b834c918d5dc557fd5c5e43: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:03:00 async_llm_engine.py:133] Finished request cmpl-14eab5ce1b834c918d5dc557fd5c5e43.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:03:00 async_llm_engine.py:564] Received request cmpl-464068e5007c467a944c18611b7882a8: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:03:01 metrics.py:341] Avg prompt throughput: 23.2 tokens/s, Avg generation throughput: 155.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:03:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:03:11 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:03:12 async_llm_engine.py:133] Finished request cmpl-464068e5007c467a944c18611b7882a8.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:03:12 async_llm_engine.py:564] Received request cmpl-868a2c5ea475471284d9a286c449e4a0: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:03:16 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 155.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:03:21 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 154.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:03:25 async_llm_engine.py:133] Finished request cmpl-868a2c5ea475471284d9a286c449e4a0.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:03:25 async_llm_engine.py:564] Received request cmpl-903be78c95ad4fe1b6595e5dc218e9e8: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:03:26 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 152.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:03:31 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 159.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:03:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:03:38 async_llm_engine.py:133] Finished request cmpl-903be78c95ad4fe1b6595e5dc218e9e8.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:03:38 async_llm_engine.py:564] Received request cmpl-45be355be0e34979a7565119edc7825e: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:03:41 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 156.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:03:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:03:51 async_llm_engine.py:133] Finished request cmpl-45be355be0e34979a7565119edc7825e.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:03:51 async_llm_engine.py:564] Received request cmpl-79545bdfea4d47ccac57eafb73937e38: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:03:51 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 154.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:03:52 async_llm_engine.py:133] Finished request cmpl-79545bdfea4d47ccac57eafb73937e38.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:03:52 async_llm_engine.py:564] Received request cmpl-af8418a0b5a740b88d6115e3ac136fa7: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:03:56 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 156.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:04:01 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:04:05 async_llm_engine.py:133] Finished request cmpl-af8418a0b5a740b88d6115e3ac136fa7.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:04:05 async_llm_engine.py:564] Received request cmpl-ec92119ac5c6422281a862e2f9654bbe: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:04:06 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 155.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:04:11 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:04:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:04:17 async_llm_engine.py:133] Finished request cmpl-ec92119ac5c6422281a862e2f9654bbe.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:04:17 async_llm_engine.py:564] Received request cmpl-021f797c74384e1990ce0bd6af711709: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:04:21 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 156.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:04:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:04:30 async_llm_engine.py:133] Finished request cmpl-021f797c74384e1990ce0bd6af711709.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:04:30 async_llm_engine.py:564] Received request cmpl-f40522a9f3314f7dade90933161d7486: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:04:30 async_llm_engine.py:133] Finished request cmpl-f40522a9f3314f7dade90933161d7486.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:04:30 async_llm_engine.py:564] Received request cmpl-ddcc7827a63847708f8abdd994834e2f: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:04:31 metrics.py:341] Avg prompt throughput: 23.2 tokens/s, Avg generation throughput: 154.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:04:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:04:41 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:04:43 async_llm_engine.py:133] Finished request cmpl-ddcc7827a63847708f8abdd994834e2f.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:04:43 async_llm_engine.py:564] Received request cmpl-5bc8ba5366ec4d59acfab6ee8b6640bf: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:04:45 async_llm_engine.py:133] Finished request cmpl-5bc8ba5366ec4d59acfab6ee8b6640bf.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:04:45 async_llm_engine.py:564] Received request cmpl-7e1700307913442298e17ae95bad62c3: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:04:46 async_llm_engine.py:133] Finished request cmpl-7e1700307913442298e17ae95bad62c3.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:04:46 async_llm_engine.py:564] Received request cmpl-c2de1a3affef4ddea9dcd9ffdb59d66f: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:04:46 metrics.py:341] Avg prompt throughput: 34.8 tokens/s, Avg generation throughput: 154.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:04:51 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:04:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:04:58 async_llm_engine.py:133] Finished request cmpl-c2de1a3affef4ddea9dcd9ffdb59d66f.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:04:58 async_llm_engine.py:564] Received request cmpl-f38567b6fbcb4b16af41668fe998c33e: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:05:00 async_llm_engine.py:133] Finished request cmpl-f38567b6fbcb4b16af41668fe998c33e.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:05:00 async_llm_engine.py:564] Received request cmpl-8df217c0b6f94d7eb621a82ad3f2331b: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:05:01 metrics.py:341] Avg prompt throughput: 23.2 tokens/s, Avg generation throughput: 154.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:05:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:05:11 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:05:13 async_llm_engine.py:133] Finished request cmpl-8df217c0b6f94d7eb621a82ad3f2331b.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:05:13 async_llm_engine.py:564] Received request cmpl-e3e39c0e31ab4e10a9e773b5eac3bc7d: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:05:16 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 156.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:05:21 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:05:26 async_llm_engine.py:133] Finished request cmpl-e3e39c0e31ab4e10a9e773b5eac3bc7d.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:05:26 async_llm_engine.py:564] Received request cmpl-de5d472fe6634674ab8a58637a79d6b7: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:05:26 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 154.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:05:29 async_llm_engine.py:133] Finished request cmpl-de5d472fe6634674ab8a58637a79d6b7.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:05:29 async_llm_engine.py:564] Received request cmpl-473fd69e30f94369a9f9a81868d2fcad: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:05:31 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 156.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:05:34 async_llm_engine.py:133] Finished request cmpl-473fd69e30f94369a9f9a81868d2fcad.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:05:34 async_llm_engine.py:564] Received request cmpl-6e4b8a110ba64d89b41cf670429aac6f: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:05:36 metrics.py:341] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 156.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:05:41 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:05:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:05:47 async_llm_engine.py:133] Finished request cmpl-6e4b8a110ba64d89b41cf670429aac6f.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:05:47 async_llm_engine.py:564] Received request cmpl-d425662a52f84c25866d113f00da812e: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:05:48 async_llm_engine.py:133] Finished request cmpl-d425662a52f84c25866d113f00da812e.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:05:48 async_llm_engine.py:564] Received request cmpl-7a6bbb3781854dd89f038cab0d52be9b: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代背景，一名神探警察遇到了一桩棘手的连环失踪案并将其侦破的故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1990, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 102193, 3837, 101177, 99315, 99633, 104834, 109075, 14777, 102927, 116484, 44934, 9370, 54926, 86312, 110786, 80642, 62926, 105278, 100613, 99577, 104413, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:05:51 metrics.py:341] Avg prompt throughput: 23.2 tokens/s, Avg generation throughput: 155.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:05:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:06:00 async_llm_engine.py:133] Finished request cmpl-7a6bbb3781854dd89f038cab0d52be9b.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:06:00 async_llm_engine.py:564] Received request cmpl-3688f557055d41c7878c3f80b1da566f: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:06:01 async_llm_engine.py:133] Finished request cmpl-3688f557055d41c7878c3f80b1da566f.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:06:01 async_llm_engine.py:564] Received request cmpl-e87342f0689b4d0dbf9b5ad62f69cb5e: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:06:01 async_llm_engine.py:133] Finished request cmpl-e87342f0689b4d0dbf9b5ad62f69cb5e.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:06:01 async_llm_engine.py:564] Received request cmpl-ce3860852e0c408b84dcf801f3928770: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:06:01 metrics.py:341] Avg prompt throughput: 32.4 tokens/s, Avg generation throughput: 154.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:06:03 async_llm_engine.py:133] Finished request cmpl-ce3860852e0c408b84dcf801f3928770.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:06:03 async_llm_engine.py:564] Received request cmpl-ae07fa9f243b4719a449f86283f6e224: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:06:06 metrics.py:341] Avg prompt throughput: 10.8 tokens/s, Avg generation throughput: 157.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:06:11 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:06:16 async_llm_engine.py:133] Finished request cmpl-ae07fa9f243b4719a449f86283f6e224.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:06:16 async_llm_engine.py:564] Received request cmpl-12174848b3f74dd7a614aa282ab41423: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:06:16 async_llm_engine.py:133] Finished request cmpl-12174848b3f74dd7a614aa282ab41423.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:06:16 async_llm_engine.py:564] Received request cmpl-0e9a472ab76543a6a4d82e7ed2072fc3: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:06:16 metrics.py:341] Avg prompt throughput: 21.6 tokens/s, Avg generation throughput: 155.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:06:17 async_llm_engine.py:133] Finished request cmpl-0e9a472ab76543a6a4d82e7ed2072fc3.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:06:17 async_llm_engine.py:564] Received request cmpl-31b709b114be4ae0b2f4d649e512ca0f: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:06:18 async_llm_engine.py:133] Finished request cmpl-31b709b114be4ae0b2f4d649e512ca0f.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:06:18 async_llm_engine.py:564] Received request cmpl-63872cef49ff4cd098190778c564ad31: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:06:21 metrics.py:341] Avg prompt throughput: 21.6 tokens/s, Avg generation throughput: 155.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:06:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 160.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:06:30 async_llm_engine.py:133] Finished request cmpl-63872cef49ff4cd098190778c564ad31.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:06:30 async_llm_engine.py:564] Received request cmpl-a0d3f1da2b9945fd9fd05f630604741c: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:06:31 metrics.py:341] Avg prompt throughput: 10.8 tokens/s, Avg generation throughput: 148.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:06:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:06:42 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 154.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:06:43 async_llm_engine.py:133] Finished request cmpl-a0d3f1da2b9945fd9fd05f630604741c.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:06:43 async_llm_engine.py:564] Received request cmpl-73d59a363c514779a705a011af8a5cc1: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:06:47 metrics.py:341] Avg prompt throughput: 10.8 tokens/s, Avg generation throughput: 152.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:06:52 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:06:56 async_llm_engine.py:133] Finished request cmpl-73d59a363c514779a705a011af8a5cc1.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:06:56 async_llm_engine.py:564] Received request cmpl-a9e9a62a949c4724bda2c0d5f673218c: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:06:57 metrics.py:341] Avg prompt throughput: 10.8 tokens/s, Avg generation throughput: 154.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:06:58 async_llm_engine.py:133] Finished request cmpl-a9e9a62a949c4724bda2c0d5f673218c.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:06:58 async_llm_engine.py:564] Received request cmpl-ea04ac7fe8614a2486fd3f1abf253a36: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:07:00 async_llm_engine.py:133] Finished request cmpl-ea04ac7fe8614a2486fd3f1abf253a36.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:07:00 async_llm_engine.py:564] Received request cmpl-c9aa0074998c4a43a74728aa338ee302: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:07:02 metrics.py:341] Avg prompt throughput: 21.6 tokens/s, Avg generation throughput: 156.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:07:07 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:07:12 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:07:13 async_llm_engine.py:133] Finished request cmpl-c9aa0074998c4a43a74728aa338ee302.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:07:13 async_llm_engine.py:564] Received request cmpl-2f203df04f12455b9822d2493ec44cde: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:07:13 async_llm_engine.py:133] Finished request cmpl-2f203df04f12455b9822d2493ec44cde.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:07:13 async_llm_engine.py:564] Received request cmpl-d00c68df601f4a8bb902c137c3fb276b: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:07:17 metrics.py:341] Avg prompt throughput: 21.6 tokens/s, Avg generation throughput: 156.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:07:22 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:07:26 async_llm_engine.py:133] Finished request cmpl-d00c68df601f4a8bb902c137c3fb276b.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:07:26 async_llm_engine.py:564] Received request cmpl-3b0b2165164b4176893fa0527a7c3665: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:07:27 metrics.py:341] Avg prompt throughput: 10.8 tokens/s, Avg generation throughput: 155.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:07:32 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:07:37 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:07:39 async_llm_engine.py:133] Finished request cmpl-3b0b2165164b4176893fa0527a7c3665.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:07:39 async_llm_engine.py:564] Received request cmpl-e264a71569b345b38aafddee13347a89: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:07:40 async_llm_engine.py:133] Finished request cmpl-e264a71569b345b38aafddee13347a89.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:07:40 async_llm_engine.py:564] Received request cmpl-27e9e38a7dae4805ae5371eca3199062: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:07:42 metrics.py:341] Avg prompt throughput: 21.6 tokens/s, Avg generation throughput: 154.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:07:47 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:07:52 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:07:53 async_llm_engine.py:133] Finished request cmpl-27e9e38a7dae4805ae5371eca3199062.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:07:53 async_llm_engine.py:564] Received request cmpl-97db8edca65c4f078a9f94a97c13f81b: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:07:53 async_llm_engine.py:133] Finished request cmpl-97db8edca65c4f078a9f94a97c13f81b.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:07:53 async_llm_engine.py:564] Received request cmpl-502c70006d97481ba819b63fae42c28a: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:07:57 metrics.py:341] Avg prompt throughput: 21.6 tokens/s, Avg generation throughput: 155.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:08:02 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:08:06 async_llm_engine.py:133] Finished request cmpl-502c70006d97481ba819b63fae42c28a.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:08:06 async_llm_engine.py:564] Received request cmpl-a3fbbfe386a34030b3c47185cf5c2c9d: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:08:07 metrics.py:341] Avg prompt throughput: 10.8 tokens/s, Avg generation throughput: 154.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:08:12 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:08:17 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:08:19 async_llm_engine.py:133] Finished request cmpl-a3fbbfe386a34030b3c47185cf5c2c9d.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:08:19 async_llm_engine.py:564] Received request cmpl-c6fbc4fd1584498d9ddb8e6db9e44dea: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:08:22 metrics.py:341] Avg prompt throughput: 10.8 tokens/s, Avg generation throughput: 155.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:08:27 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:08:31 async_llm_engine.py:133] Finished request cmpl-c6fbc4fd1584498d9ddb8e6db9e44dea.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:08:32 async_llm_engine.py:564] Received request cmpl-03579cacc0da46aa9121ebeec9eebeb4: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:08:32 metrics.py:341] Avg prompt throughput: 10.8 tokens/s, Avg generation throughput: 154.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:08:37 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:08:42 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:08:44 async_llm_engine.py:133] Finished request cmpl-03579cacc0da46aa9121ebeec9eebeb4.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:08:44 async_llm_engine.py:564] Received request cmpl-61586b49339b40328f54d26433203462: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:08:47 metrics.py:341] Avg prompt throughput: 10.8 tokens/s, Avg generation throughput: 154.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:08:52 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:08:57 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:08:57 async_llm_engine.py:133] Finished request cmpl-61586b49339b40328f54d26433203462.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:08:57 async_llm_engine.py:564] Received request cmpl-26eab9730c5a4f92a5b572354fb2a737: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:08:58 async_llm_engine.py:133] Finished request cmpl-26eab9730c5a4f92a5b572354fb2a737.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:08:58 async_llm_engine.py:564] Received request cmpl-452708357d6e4348a7d5848499fa3f2c: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:08:58 async_llm_engine.py:133] Finished request cmpl-452708357d6e4348a7d5848499fa3f2c.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:08:58 async_llm_engine.py:564] Received request cmpl-abb81c462e0f452dbf944d1ce47d7b36: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:09:00 async_llm_engine.py:133] Finished request cmpl-abb81c462e0f452dbf944d1ce47d7b36.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:09:00 async_llm_engine.py:564] Received request cmpl-ff04945bbca54af0b8cc61dc6c819d8c: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:09:02 metrics.py:341] Avg prompt throughput: 43.2 tokens/s, Avg generation throughput: 153.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:09:07 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:09:12 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:09:13 async_llm_engine.py:133] Finished request cmpl-ff04945bbca54af0b8cc61dc6c819d8c.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:09:13 async_llm_engine.py:564] Received request cmpl-d1f1dee3cac149e5988407ac6eb699c0: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:09:17 metrics.py:341] Avg prompt throughput: 10.8 tokens/s, Avg generation throughput: 155.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:09:22 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:09:26 async_llm_engine.py:133] Finished request cmpl-d1f1dee3cac149e5988407ac6eb699c0.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:09:26 async_llm_engine.py:564] Received request cmpl-532fcc91b1004516934a881886d51456: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:09:27 metrics.py:341] Avg prompt throughput: 10.8 tokens/s, Avg generation throughput: 155.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:09:32 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:09:37 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:09:39 async_llm_engine.py:133] Finished request cmpl-532fcc91b1004516934a881886d51456.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:09:39 async_llm_engine.py:564] Received request cmpl-952c88ace637458ebed5ab8c029c0761: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:09:41 async_llm_engine.py:133] Finished request cmpl-952c88ace637458ebed5ab8c029c0761.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:09:41 async_llm_engine.py:564] Received request cmpl-f4a0c49374744268aa4f90770ae624ed: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:09:42 metrics.py:341] Avg prompt throughput: 21.6 tokens/s, Avg generation throughput: 154.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:09:44 async_llm_engine.py:133] Finished request cmpl-f4a0c49374744268aa4f90770ae624ed.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:09:44 async_llm_engine.py:564] Received request cmpl-5d09e6232c2c4b33ae631aa27510be15: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:09:44 async_llm_engine.py:133] Finished request cmpl-5d09e6232c2c4b33ae631aa27510be15.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:09:45 async_llm_engine.py:564] Received request cmpl-4407215cc7624833966f2ce56947fb1c: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:09:45 async_llm_engine.py:133] Finished request cmpl-4407215cc7624833966f2ce56947fb1c.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:09:45 async_llm_engine.py:564] Received request cmpl-0fc4c4015b8b4dc9bcb6518fc5c07e0d: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:09:47 metrics.py:341] Avg prompt throughput: 32.4 tokens/s, Avg generation throughput: 155.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:09:52 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:09:57 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:09:58 async_llm_engine.py:133] Finished request cmpl-0fc4c4015b8b4dc9bcb6518fc5c07e0d.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:09:58 async_llm_engine.py:564] Received request cmpl-ce9f62f1166842b78bbd7a7cb2267fe6: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:09:58 async_llm_engine.py:133] Finished request cmpl-ce9f62f1166842b78bbd7a7cb2267fe6.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:09:58 async_llm_engine.py:564] Received request cmpl-7b7ffa52956943b2a5872f900b8cc78c: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:10:02 metrics.py:341] Avg prompt throughput: 21.6 tokens/s, Avg generation throughput: 153.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:10:03 async_llm_engine.py:133] Finished request cmpl-7b7ffa52956943b2a5872f900b8cc78c.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:10:03 async_llm_engine.py:564] Received request cmpl-b3c83e541e6a44aabcb178a7e7ae0e8c: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:10:07 metrics.py:341] Avg prompt throughput: 10.8 tokens/s, Avg generation throughput: 153.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:10:12 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 150.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:10:16 async_llm_engine.py:133] Finished request cmpl-b3c83e541e6a44aabcb178a7e7ae0e8c.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:10:16 async_llm_engine.py:564] Received request cmpl-9b89269fe50c4b7aae14ff7ecff9f310: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:10:17 metrics.py:341] Avg prompt throughput: 10.8 tokens/s, Avg generation throughput: 154.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:10:22 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:10:27 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:10:29 async_llm_engine.py:133] Finished request cmpl-9b89269fe50c4b7aae14ff7ecff9f310.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:10:29 async_llm_engine.py:564] Received request cmpl-72b33491519e4d8c9483deda5876ed1c: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:10:32 metrics.py:341] Avg prompt throughput: 10.8 tokens/s, Avg generation throughput: 152.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:10:37 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:10:41 async_llm_engine.py:133] Finished request cmpl-72b33491519e4d8c9483deda5876ed1c.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:10:41 async_llm_engine.py:564] Received request cmpl-99747c0b789e472094adca5599dc7ae9: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:10:42 metrics.py:341] Avg prompt throughput: 10.8 tokens/s, Avg generation throughput: 155.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:10:47 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:10:52 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:10:54 async_llm_engine.py:133] Finished request cmpl-99747c0b789e472094adca5599dc7ae9.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:10:54 async_llm_engine.py:564] Received request cmpl-4245950210584f6c979bbb73dee1a60f: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:10:57 metrics.py:341] Avg prompt throughput: 10.8 tokens/s, Avg generation throughput: 155.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:11:02 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:11:07 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:11:07 async_llm_engine.py:133] Finished request cmpl-4245950210584f6c979bbb73dee1a60f.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:11:07 async_llm_engine.py:564] Received request cmpl-2c09121012ba4908a24a52fe818d5807: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:11:12 metrics.py:341] Avg prompt throughput: 10.8 tokens/s, Avg generation throughput: 157.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:11:17 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:11:20 async_llm_engine.py:133] Finished request cmpl-2c09121012ba4908a24a52fe818d5807.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:11:20 async_llm_engine.py:564] Received request cmpl-05cc8d356c474a69920e15dcec03126b: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:11:20 async_llm_engine.py:133] Finished request cmpl-05cc8d356c474a69920e15dcec03126b.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:11:20 async_llm_engine.py:564] Received request cmpl-ab726187dc614412b010aec1b9934280: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:11:21 async_llm_engine.py:133] Finished request cmpl-ab726187dc614412b010aec1b9934280.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:11:21 async_llm_engine.py:564] Received request cmpl-d57706e095444c65aab68488281bf35c: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:11:21 async_llm_engine.py:133] Finished request cmpl-d57706e095444c65aab68488281bf35c.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:11:22 async_llm_engine.py:564] Received request cmpl-d9820b7da99c4908afcccc3a9c516840: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:11:22 metrics.py:341] Avg prompt throughput: 43.2 tokens/s, Avg generation throughput: 152.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:11:22 async_llm_engine.py:133] Finished request cmpl-d9820b7da99c4908afcccc3a9c516840.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:11:22 async_llm_engine.py:564] Received request cmpl-041dc4918f8c40c18a775598ab2fea4b: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:11:24 async_llm_engine.py:133] Finished request cmpl-041dc4918f8c40c18a775598ab2fea4b.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:11:24 async_llm_engine.py:564] Received request cmpl-aa33aec50008401b81b2373c50073fb8: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:11:24 async_llm_engine.py:133] Finished request cmpl-aa33aec50008401b81b2373c50073fb8.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:11:24 async_llm_engine.py:564] Received request cmpl-82ebd247f16f49dcae3a10931e3a6f48: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n古代背景，皇家侍卫和公主历经层层考验，突破身份桎梏的爱情故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1994, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 102640, 102193, 3837, 109325, 107010, 99417, 33108, 105454, 108580, 109188, 106290, 3837, 101969, 101294, 120325, 120607, 110066, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:11:27 metrics.py:341] Avg prompt throughput: 32.4 tokens/s, Avg generation throughput: 154.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:11:32 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:11:37 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 155.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:11:37 async_llm_engine.py:133] Finished request cmpl-82ebd247f16f49dcae3a10931e3a6f48.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:11:37 async_llm_engine.py:564] Received request cmpl-4fddedcc358b4aa49955d3c2c10088e4: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代玄幻背景，在一所驯服神兽的魔法学校中，围绕着三个学生小伙伴发生的奇幻冒险故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 104731, 100320, 102193, 96050, 108492, 116819, 43209, 99315, 101336, 9370, 106083, 100344, 15946, 3837, 102074, 99164, 101124, 99720, 103291, 106806, 117851, 107658, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:11:38 async_llm_engine.py:133] Finished request cmpl-4fddedcc358b4aa49955d3c2c10088e4.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:11:38 async_llm_engine.py:564] Received request cmpl-8a5535dda48643afa1661711cd50850c: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代玄幻背景，在一所驯服神兽的魔法学校中，围绕着三个学生小伙伴发生的奇幻冒险故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 104731, 100320, 102193, 96050, 108492, 116819, 43209, 99315, 101336, 9370, 106083, 100344, 15946, 3837, 102074, 99164, 101124, 99720, 103291, 106806, 117851, 107658, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:11:42 metrics.py:341] Avg prompt throughput: 24.0 tokens/s, Avg generation throughput: 156.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:11:43 async_llm_engine.py:133] Finished request cmpl-8a5535dda48643afa1661711cd50850c.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:11:43 async_llm_engine.py:564] Received request cmpl-0d63c7c05a3545d7bb60a82c42e1f568: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代玄幻背景，在一所驯服神兽的魔法学校中，围绕着三个学生小伙伴发生的奇幻冒险故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 104731, 100320, 102193, 96050, 108492, 116819, 43209, 99315, 101336, 9370, 106083, 100344, 15946, 3837, 102074, 99164, 101124, 99720, 103291, 106806, 117851, 107658, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:11:47 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 156.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:11:52 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 156.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:11:56 async_llm_engine.py:133] Finished request cmpl-0d63c7c05a3545d7bb60a82c42e1f568.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:11:56 async_llm_engine.py:564] Received request cmpl-f9cc1336e17940a6a85b8539947887cc: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代玄幻背景，在一所驯服神兽的魔法学校中，围绕着三个学生小伙伴发生的奇幻冒险故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 104731, 100320, 102193, 96050, 108492, 116819, 43209, 99315, 101336, 9370, 106083, 100344, 15946, 3837, 102074, 99164, 101124, 99720, 103291, 106806, 117851, 107658, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:11:56 async_llm_engine.py:133] Finished request cmpl-f9cc1336e17940a6a85b8539947887cc.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:11:56 async_llm_engine.py:564] Received request cmpl-30c37f9f2f8747a8821b2c903fb71761: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代玄幻背景，在一所驯服神兽的魔法学校中，围绕着三个学生小伙伴发生的奇幻冒险故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 104731, 100320, 102193, 96050, 108492, 116819, 43209, 99315, 101336, 9370, 106083, 100344, 15946, 3837, 102074, 99164, 101124, 99720, 103291, 106806, 117851, 107658, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:11:57 metrics.py:341] Avg prompt throughput: 24.0 tokens/s, Avg generation throughput: 153.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:12:02 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:12:06 async_llm_engine.py:133] Finished request cmpl-30c37f9f2f8747a8821b2c903fb71761.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:55262 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 06-21 23:12:06 async_llm_engine.py:564] Received request cmpl-758f18e13d854ac68407bf6d3b2f24f0: prompt: '<|im_start|>system\\n你是一个熟读各类小说的专家，请你根据要求写一段800字左右的小说。<|im_end|>\\n<|im_start|>user\\n现代玄幻背景，在一所驯服神兽的魔法学校中，围绕着三个学生小伙伴发生的奇幻冒险故事。<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1988, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 100089, 57553, 104010, 104032, 9370, 101057, 37945, 56568, 100345, 101882, 61443, 104383, 23, 15, 15, 18600, 101081, 104006, 36587, 1773, 151645, 198, 151644, 872, 198, 100390, 104731, 100320, 102193, 96050, 108492, 116819, 43209, 99315, 101336, 9370, 106083, 100344, 15946, 3837, 102074, 99164, 101124, 99720, 103291, 106806, 117851, 107658, 101108, 1773, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 06-21 23:12:07 metrics.py:341] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 156.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:12:12 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 158.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:12:16 async_llm_engine.py:167] Aborted request cmpl-758f18e13d854ac68407bf6d3b2f24f0.\n",
      "INFO 06-21 23:12:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 44.6 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:12:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:12:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:12:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:13:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:13:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:13:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:13:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:13:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:13:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:14:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:14:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:14:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:14:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:14:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 06-21 23:14:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "!python -m vllm.entrypoints.openai.api_server --model ./merged_model  --served-model-name Qwen2-1.5B-Instruct-lora --max-model-len=2048\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
